#!/usr/bin/env perl
# Copyright [2018-2019] EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


=head1 NAME

dump_pipeline - script to dump pipeline database to a .sql file, create and populate a vega db

=head1 SYNOPSIS

dump_pipeline [options]

Options:

    --conffile, --conf=FILE             read script parameters from FILE
                                        (default: conf/Conversion.ini)
    --pipedbname=NAME                   use pipeline database NAME
    --pipehost=HOST                     use pipeline database host HOST
    --pipeport=PORT                     use pipeline database port PORT
    --pipeuser=USER                     use pipeline database user USER
    --pipepass=PASS                     use pipeline database password PASS

    --dbname, db_name=NAME              use database NAME for the pipeline dump
    --host, --dbhost, --db_host=HOST    use database host HOST for the pipeline dump db
    --port, --dbport, --db_port=PORT    use database port PORT for the pipeline dump db
    --user, --dbuser, --db_user=USER    use database username USER for the pipeline dump db
    --pass, --dbpass, --db_pass=PASS    use database passwort PASS for the pipeline dump db

    --logfile, --log=FILE               log to FILE (default: *STDOUT)
    --logpath=PATH                      write logfile to PATH (default: .)
    -v, --verbose                       verbose logging (default: false)
    -i, --interactive=0|1               run script interactively (default: true)
    -n, --dry_run, --dry=0|1            don't write results to database
    -h, --help, -?                      print help (this message)

    -no_feature=1                       don't dump dna_ and protein_ align features
    -sql_dump_location=DIR              location of SQL dump (defaults to /tmp/vega_prep)
    -file_name=FILE                     name of SQL file (defaults to species_date_create.sql)

=head1 DESCRIPTION

This script uses MySQLdump to read a pipeline database (InnoDB) into a file that can be used,
either by this script or subsequently, to create a new Vega (MyISAM) database. The user is
prompted as to whether to automatically create and populate the new Vega db.

By default only the structure for tables will be read into the file - the exceptions are tables
defined in the HEREDOC at the end of the script - tables to be completely ignored ('i'), 
tables where all data is also dumped ('d'), and tables where the features being dumped are
restricted by analysis_id ('analysis_id'). This last filter is just for *_raw features for 
*_align_feature tables - if you want to transfer all align_features then just edit the HEREDOC.
Note that earlier versions of this script (eg 1.6) attempted to restrict the transfer of all
features on analysis_id and seq_region_id - this worked for small database but fell over with
human.

Move the table definitions at the end to dump halfwise genes once we worked out what to do with
them (they are part of the analysis pipeline, showing pfam domain matches from pfam HMM alignments
originally located by swissprot blast hits.)

Transfer of features from the dna_ and protein_align_feature tables can be completely
prevented using the -no_feature option to the script.


=head1 AUTHOR

Steve Trevanion <st3@sanger.ac.uk>

=head1 CONTACT

Post questions to the EnsEMBL development list ensembl-dev@ebi.ac.uk

=cut

use strict;
use warnings;
no warnings 'uninitialized';

use FindBin qw($Bin);
use vars qw($SERVERROOT);

BEGIN {
  $SERVERROOT = "$Bin/../../../..";
  unshift(@INC, "$SERVERROOT/ensembl-otter/modules");
  unshift(@INC, "$SERVERROOT/ensembl/modules");
  unshift(@INC, "$SERVERROOT/ensembl-variation/modules");
  unshift(@INC, "$SERVERROOT/bioperl-live");
  unshift(@INC, "modules");
}

use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;
use Data::Dumper;
use Bio::EnsEMBL::Utils::ConversionSupport;

$| = 1;

my $support = new Bio::EnsEMBL::Utils::ConversionSupport($SERVERROOT);

### PARALLEL # $support ###

# parse options
$support->parse_common_options(@_);
$support->parse_extra_options(
  'pipedbname=s',
  'pipehost=s',
  'pipeport=s',
  'pipeuser=s',
  'no_feature=s',
  'sql_dump_location=s',
  'file_name=s',
  $support->get_loutre_params,
);
$support->allowed_params(
  'pipedbname',
  'pipehost',
  'pipeport',
  'pipeuser',
  'no_feature',
  'sql_dump_location',
  'file_name',
  $support->get_common_params,
);
$support->check_required_params(
  'pipedbname',
  'pipehost',
  'pipeport',
  'pipeuser',
);
if ($support->param('help') or $support->error) {
  warn $support->error if $support->error;
  pod2usage(1);
}

#set defaults if not specified otherwise
if (! $support->param('sql_dump_location')) {
  $support->param('sql_dump_location','/tmp/vega_prep');
}
my $date = strftime "%Y_%m_%d", localtime;
if (! $support->param('file_name')) {
  my $file_name = $support->param('pipedbname').'_'.$date.'_create.sql';
  $support->param('file_name',$file_name);
}

# ask user to confirm parameters to proceed
$support->confirm_params;

# get log filehandle and print heading and parameters to logfile
$support->init_log;

my ($file,%mysql_commands,$create_db,$dbtype);
### PRE # $file %mysql_commands $create_db $dbtype # ###

#are we going to create the database and populate it from the sql dump
$create_db = 0;
if ($support->user_proceed("Do you want to automatically create and populate the vega database from the sql dump?\n")) {
  $create_db = 1;
}

#for debugging, are we dumping align_features (can be slow) or just the table structure
my $no_feature = $support->param('no_feature') || '';

#character set
my $character_set='latin1';

#database type
$dbtype='MyISAM';

#open file to dump the sql into
$file = $support->param('sql_dump_location') . '/' . $support->param('file_name');

# connect to pipeline database and get adaptors
my $pdba = $support->get_database('core','pipe');
my $pdbh = $pdba->dbc->db_handle;

# set up some filters - only want non *_raw_|*_new_ features for  *_align_features
my %analysis_ids;
my $gtn = $pdbh->prepare(qq(
              SELECT distinct analysis_id, logic_name
              FROM analysis
    ));
$gtn->execute();
while (my ($analysis_id,$logic_name) = $gtn->fetchrow){
  #exclude for align_features
  next if ($logic_name=~/\_raw|_new$/);
  foreach my $t (qw(analysis protein_align_feature dna_align_feature)) {
    push @{$analysis_ids{$t}{'analysis_id'}}, $analysis_id;
  }
}

#check we've only got Halfwise gene xrefs
$gtn = $pdbh->prepare(qq(SELECT distinct ensembl_object_type FROM object_xref));
$gtn->execute();
while (my $type = $gtn->fetchrow){
  if ($type ne 'Gene') {
    $support->log_warning("There are xrefs on objects ($type) other than genes, please find out what they are\n");
  }
}
$gtn = $pdbh->prepare(qq(
   SELECT distinct a.logic_name
     FROM object_xref ox, gene g, analysis a
    WHERE ox.ensembl_id = g.gene_id
      AND g.analysis_id = a.analysis_id
      AND ox.ensembl_object_type = 'Gene'));
$gtn->execute();
while (my $ln = $gtn->fetchrow){
  if ($ln ne 'Halfwise') {
    $support->log_warning("There are xrefs on objects other than Halfwise genes ($ln), please find out what they are\n");
  }
}

#warn Dumper(\%analysis_ids);

#read all tables;
my %table_constraints;
my $pipe_db = $support->param('pipedbname');
map { $_ =~ s/`//g;  $_ =~ s/$pipe_db.//g; $table_constraints{$_} = 's'; } $pdbh->tables;

#warn Dumper(\%table_constraints);

#read details of constrained tables from HEREDOC
my $txt = &constraints;
#warn $txt;
TABLE:
foreach my $line (split(/\n/,$txt)){
  next if ($line =~ /^\s*$/);
  next if ($line =~ /^\#/);
  if ($line=~/^(.+)\#/){
    $line=$1;
  }

  my ($table,$constraint) = split(/\s+/,$line);
#    $table =  $support->param('pipedbname').'.'.$table;

  #sanity check
  if ($table && (! exists($table_constraints{$table}))) {
    $support->log_warning("You have definitions for a table ($table) that is not found in the pipeline database. Skipping\n\n");
    next TABLE;
  }

  #skip tables to ignore
  if ($constraint eq 'i') {
    $table_constraints{$table} = 'i';
    next TABLE;
  }
		
  #if we don't want to dump align_features features then set type to 's'
  if ($table=~/align_feature$/ && $no_feature) {
    $support->log("\'no_feature\' option used: skipping features from $table\n");
    $table_constraints{$table} = 's';
    next TABLE;
  }

  #are any constraints by analysis_id defined
  if ($analysis_ids{$table}) {
    if ($constraint eq 'analysis_id') {
      $table_constraints{$table} = $analysis_ids{$table};
    }
    else {
      $support->log_warning("Don't understand what to do with table $table\n");
    }
    next TABLE;
    }

  if ($constraint eq 'd') {
    $table_constraints{$table} = 'd';
  }
  #further sanity check
  elsif ($constraint ne 'a') {
    $support->log_warning("Constraint ($constraint) for table ($table)not understood. Skipping\n\n");
    next TABLE;
  }
}

#warn Dumper(\%table_constraints);

#Do some logging		
my $log = "The following tables will be ignored (ie not put into Vega):\n";
foreach my $table (keys %table_constraints) {
  $log .="\t$table\n" if ($table_constraints{$table} eq 'i');
}
$log .= "The following tables will be dumped with all their data:\n";
foreach my $table (keys %table_constraints) {
  $log .= "\t$table\n" if ($table_constraints{$table} eq 'd');
}

$log .= "The following tables will be dumped with some of their data:\n";
foreach my $table (keys %table_constraints) {
  if (ref($table_constraints{$table}) eq 'HASH') {
    foreach my $cons (keys %{$table_constraints{$table}}) {
      $log .= "\t$table (constrained on $cons)\n";
    }
  }
}

$log .= "The rest of the pipeline tables will be copied just with their structure (no data):\n";
foreach my $table (keys %table_constraints) {
  $log .= "\t$table\n" if ($table_constraints{$table} eq 's');
}

$support->log($log);

unless ($support->user_proceed("$log\nDo you want to proceed ?")) {
  exit;
}

#########################
# create mysql commands #
#########################

#initialise mysqldump statements
my $cs;
if(my $character_set) {$cs="--default-character-set=\"$character_set\"";}
my $sei;
if(my $opt_c) {$sei='--skip-extended-insert';}
my $user   = $support->param('pipeuser');
my $dbname = $support->param('pipedbname');
my $host   = $support->param('pipehost');
my $port   = $support->param('pipeport');

my $mcom   = "mysqldump --opt --skip-lock-tables $sei $cs --single-transaction -q -u $user -P $port -h $host $dbname";

#create statements
while (my ($table,$condition) = each (%table_constraints) ) {
  $mysql_commands{$table} = [];
  next if ($condition eq 'i');
  if ($condition eq 's') {
    push @{$mysql_commands{$table}},"$mcom -d $table";
  }
  elsif ($condition eq 'd') {
    push @{$mysql_commands{$table}},"$mcom $table";
  }
  elsif (ref($table_constraints{$table}) eq 'HASH') {
    foreach my $cons (keys %{$table_constraints{$table}}) {
      my $ids = join ',',@{$table_constraints{$table}{$cons}};
      my $extra = qq( --where "$cons in ($ids)" );
      push @{$mysql_commands{$table}},"$mcom $extra $table";
    }
  }
}

##################
# do the dumping #
##################
	
#warn Dumper(\@mysql_commands);

my @mysql_tables = keys %mysql_commands;

#create and populate vega db
if ($create_db && (!$support->param('dry_run'))) {
  my $user   = $support->param('user');
  my $dbname = $support->param('dbname');
  my $host   = $support->param('host');
  my $port   = $support->param('port');
  my $pass   = $support->param('pass');

  $support->log("Creating new Vega database $dbname on $host\n");
  my $mysql = "mysql -u $user -P $port -p$pass -h $host -e 'create database $dbname'";
  `$mysql`;
}
 
### RUN # @mysql_tables ###

foreach my $table (@mysql_tables) {
  my $filename = "$file-$table";
  open(OUT,">$filename") || die "cannot open $filename";
  if (!$support->param('dry_run')) {
    foreach my $command (@{$mysql_commands{$table}}) {
      warn "command: $command\n";
      open(MYSQL,"$command |") || die "cannot open mysql";
      my $enable;
      my $flag_disable;
      while (<MYSQL>) {
        s/(TYPE|ENGINE)=(\w+)/$1=$dbtype/;
        if (/ALTER\sTABLE\s\S+\sENABLE\sKEYS/){
    $enable=$_;
        }
        elsif (/ALTER\sTABLE\s\S+\sDISABLE\sKEYS/){
    if(!$flag_disable){
      # only write once
      $flag_disable=1;
      print OUT;
    }
        }
        else {
    print OUT;
        }
      }
      print OUT $enable if ($enable);
      close(MYSQL);
    }
    $support->log("SQL for $table dumped to $filename\n");
  } else {
    $support->log("\nNo SQL dumped since this is a dry run\n");
  }
  close OUT;
  
  if ($create_db && (!$support->param('dry_run'))) {
    $support->log("Populating $dbname\n");
    my $user   = $support->param('user');
    my $dbname = $support->param('dbname');
    my $host   = $support->param('host');
    my $port   = $support->param('port');
    my $pass   = $support->param('pass');
    my $mysql = "mysql -u $user -P $port -p$pass -h $host $dbname < $filename";
    `$mysql`;
  }
}

### POST ###
### END ###

$support->finish_log;

#######################################################################
# define the contraints on tables where data is to be transferred     #
# all tables not specified here will have only their structure copied #
#######################################################################

# All tables are by default dumped with just structure. Tables for which data is also to be dumped
# are defined here [d] as are those to be completely ignored [i]. Tables constrained on analysis_id
# are also defined

sub constraints {
  my $txt;
  $txt=<<ENDOFTEXT;
dna_align_feature_history        i
hit_description                  i
input_id_analysis                i
input_id_seq_region              i
input_id_type_analysis           i
job                              i
job_status                       i
protein_align_feature_history    i
rule_conditions                  i
rule_goal                        i

assembly                         d
seq_region                       d
seq_region_attrib                d
meta_coord                       d
coord_system                     d
dna                              d
dna_align_feature                analysis_id
protein_align_feature            analysis_id
analysis                         analysis_id
attrib_type                      d
meta                             d
prediction_exon                  d
prediction_transcript            d
repeat_consensus                 d
repeat_feature                   d
simple_feature                   d
ENDOFTEXT
  return $txt;
}

#add these tables to dump halfwise
#object_xref                      d
#xref                             d
#gene                             d
#transcript                       d
#exon_transcript                  d
#exon                             d
