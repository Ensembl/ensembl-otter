#!/usr/local/bin/perl -w

### backup_otter_submissions_db from otterslave and archive on cbi1
### running this script as a cron job on cbi1

use strict;
use DBI;
use File::Path 'rmtree';

{
    umask(002);
    {
        my $path = $ENV{'PATH'};
        $ENV{'PATH'} = "/usr/local/bin:/usr/apps/bin:$path";
    }

    my $user = 'ottadmin';
    my $host = 'otterslave';
    my $port = '3311';
    # Can add password with netrc if needed
    my $password = 'lutralutra';
    my $base_dir = '/nfs/disk100/humpub/data/mysql_backup/otter_submissions';
    my $archive_dir = '/nfs/disk100/humpub/data/mysql_backup/otter_submissions/OTTER_SUBMISSIONS_ARCHIVE';
    cleanup_old_otter_dbs($base_dir);
    my $date_dir = date_dir();
    my $working_dir = "$base_dir/$date_dir";
    mkdir($working_dir) or die "Can't create '$working_dir' directory : $!";
    chdir($working_dir) or die "Can't chdir to '$working_dir' : $!";
    
    my $dbh = DBI->connect(
        "DBI:mysql:host=otterslave:port=3311",
        $user, $password, {RaiseError => 1});
    my $dbs = list_otter_databases($dbh);
  
#    my $mysql_dump = "mysqldump --opt --single-transaction --quick --user=$user --password=$password --host=$host --port=$port ";
    my $mysql_dump = "mysqldump --opt --quick --user=$user --password=$password --host=$host --port=$port ";
    
    my $err_count = 0;
    my $backup_start = time;
    foreach my $db_name (@$dbs) {
        eval {
            print STDERR "Backing up '$db_name'\n";
            $dbh->do("use $db_name");
            check_database_not_too_big($dbh);
            backup_otter_database($dbh, $mysql_dump, $db_name);
        };
        if ($@) {
            $err_count++;
            print STDERR $@;
        }
    }

    my $backup_end = time;
    printf STDERR "otter backup took %d minutes\n", ($backup_end - $backup_start) / 60;
    
    # tar and gzip the working directory and delete the working_dir
    # archive($archive_dir,$working_dir,$date_dir);

    # All errors reflected in the exit status
    exit $err_count;
}

sub archive {

    my ($archive_dir,$working_dir,$date_dir) = @_;
    cleanup_old_otter_dbs($archive_dir);
    my $copy_start = time;
    my $copy = "tar -cf - $working_dir | gzip -9 - > $archive_dir/$date_dir.tar.gz ";
    if (system($copy) == 0) {
        rmtree($working_dir);
    } else {
        print STDERR "ERROR: exit $? from copy command: '$copy'\n";
    }
    my $copy_end = time;
    printf STDERR "otter copy took %.0f minutes\n", ($copy_end - $copy_start) / 60;
    
}

sub check_database_not_too_big {
    my( $dbh ) = @_;
    
    my $row_count = 0;
    foreach my $table (qw{
        dna_align_feature
        protein_align_feature
        simple_feature
        })
    {
        eval{
            my $sth = $dbh->prepare(qq{ SELECT count(*) FROM $table });
            $sth->execute;
            my ($rows) = $sth->fetchrow;
            $row_count += $rows;
        };
        warn $@ if $@;
    }
    my $max_rows = 100_000;
    if ($row_count > $max_rows) {
        die "ERROR: Database has $row_count rows in feature tables: limit is $max_rows\n";
    }
}

sub backup_otter_database {
    my( $dbh, $mysql_dump, $db_name ) = @_;
    
    my $sth = $dbh->prepare("show tables");
    $sth->execute;
    my( @tables_to_backup );
    while (my ($tab) = $sth->fetchrow) {
        # The DNA table has a very large amount of data
        # in it, but we don't need to back it up.
        next if $tab eq 'dna';
        push(@tables_to_backup, $tab);
    }
    
    my $sql_file = "$db_name.sql";
    my $dump_command = "$mysql_dump $db_name @tables_to_backup > $sql_file";
    system($dump_command) == 0 or die "ERROR: exit $? running '$dump_command'\n";
}

sub list_otter_databases {
    my( $dbh ) = @_;
    
    my $sth = $dbh->prepare('show databases');
    $sth->execute;
    
    my $dbs = [];
    while (my ($db_name) = $sth->fetchrow) {
        if ($db_name =~ /^(otter|submissions)/i) {
            push(@$dbs, $db_name);
        } else {
            print STDERR "Skipping database '$db_name'\n";
        }
    }
    return $dbs;
}

sub date_dir {
    my( $dbh ) = @_;
    
    my ($year, $mon, $mday, $hour, $min, $sec) = (localtime)[5,4,3,2,1,0];
    return sprintf("otter_sub_backup_%04d-%02d-%02d_%02d-%02d-%02d",
        $year + 1900, $mon + 1, $mday, $hour, $min, $sec);
}

sub cleanup_old_otter_dbs {
    my( $dir ) = @_;
    
    opendir BACKUPS, $dir or die "Can't opendir '$dir' : $!";
    my @all = sort grep /^otter_sub_backup_/, readdir BACKUPS;
    closedir BACKUPS;
    
    # Keep the most recent 24
    foreach my $i (01..24) {
        my $backup = pop(@all);
        warn "Keeping $i: $backup\n";
        last unless @all;
    }
    
    my( %seen_month );
    # Keep one from every month
    foreach my $backup (@all) {
        my ($year_month) = $backup =~ /(\d{4}-\d\d)/;
        if ($seen_month{$year_month}) {
            print STDERR "Deleteting old backup: $backup\n";
            rmtree("$dir/$backup");
        } else {
            print STDERR "Keeping: $backup\n";
            $seen_month{$year_month} = $backup;
        }
    }
}

__END__

=head1 NAME - backup_otter_submissions_db

=head1 AUTHOR

James Gilbert B<email> jgrg@sanger.ac.uk

modified for lutra replicated servers by 

Sindhu K. Pillai B<email> sp1@sanger.ac.uk
