#!/usr/bin/env perl
# Copyright [2018-2019] EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


use warnings;


=head1 NAME - backup_otter_submissions_db

=head1 SYNOPSIS

Archive otterlive to slow storage

(This script runs as a cron job from humpub@cbi4h)

=head1 CAVEATS

Note that the DBA team now also take full backups.  Otterlive's are
nightly.

=cut


use strict;
use DBI;
use File::Path 'rmtree';
use Net::Netrc;
use Hum::Conf 'WAREHOUSE_MYSQL';

{
    umask(002);

    my $host = 'otterlive';
    my $port = '3324';
    my $mach = Net::Netrc->lookup($host);
    my $user = $mach->login;
    my $password = $mach->password;

    my $dbh = DBI->connect(
        "DBI:mysql:host=$host:port=$port",
        $user, $password, {RaiseError => 1});
    my $dbs = list_otter_databases($dbh);

    my $base_dir = "$WAREHOUSE_MYSQL/otter_submissions";
    my $archive_dir = "$base_dir/OTTER_SUBMISSIONS_ARCHIVE";

    cleanup_old_otter_dbs($base_dir);
    my $date_dir = date_dir();
    my $working_dir = "$base_dir/$date_dir";
    mkdir($working_dir) or die "Can't create '$working_dir' directory : $!";
    chdir($working_dir) or die "Can't chdir to '$working_dir' : $!";
  
    # As all tables are Innodb skip locking with single transaction would give
    # a consistent backup
    my $mysql_dump = "mysqldump --skip-lock-tables --single-transaction --user=$user --password=$password --host=$host --port=$port ";
    
    my $err_count = 0;
    my $backup_start = time;
    foreach my $db_name (@$dbs) {
        eval {
            print STDERR "Backing up '$db_name'\n";
            $dbh->do("use $db_name");
            backup_otter_database($dbh, $mysql_dump, $db_name);
        };
        if ($@) {
            $err_count++;
            print STDERR $@;
        }
    }

    my $backup_end = time;
    printf STDERR "otter backup took %d minutes\n", ($backup_end - $backup_start) / 60;

    # tar and gzip the working directory and delete the working_dir
    # archive($archive_dir,$working_dir,$date_dir);

    # All errors reflected in the exit status
    exit $err_count;
}

sub archive {

    my ($archive_dir,$working_dir,$date_dir) = @_;
    cleanup_old_otter_dbs($archive_dir);
    my $copy_start = time;
    my $copy = "tar -cf - $working_dir | gzip -9 - > $archive_dir/$date_dir.tar.gz ";
    if (system($copy) == 0) {
        rmtree($working_dir);
    } else {
        print STDERR "ERROR: exit $? from copy command: '$copy'\n";
    }
    my $copy_end = time;
    printf STDERR "otter copy took %.0f minutes\n", ($copy_end - $copy_start) / 60;

    return;
}

sub backup_otter_database {
    my( $dbh, $mysql_dump, $db_name ) = @_;
    
    my $sth = $dbh->prepare("show tables");
    $sth->execute;
    my( @tables_to_backup );
    while (my ($tab) = $sth->fetchrow) {
        # The DNA table has a very large amount of data
        # in it, but we don't need to back it up.
        next if $tab eq 'dna';
        push(@tables_to_backup, $tab);
    }
    
    my $sql_file = "$db_name.sql";
    my $dump_command = "$mysql_dump $db_name @tables_to_backup > $sql_file";
    system($dump_command) == 0 or die "ERROR: exit $? running '$dump_command'\n";

    return;
}

sub list_otter_databases {
    my( $dbh ) = @_;
    
    my $sth = $dbh->prepare('show databases');
    $sth->execute;
    
    my $dbs = [];
    while (my ($db_name) = $sth->fetchrow) {
        if ($db_name =~ /^(otter|loutre|submissions|act2)/i) {
            push(@$dbs, $db_name);
        } else {
            print STDERR "Skipping database '$db_name'\n";
        }
    }
    return $dbs;
}

sub date_dir {
    my( $dbh ) = @_;
    
    my ($year, $mon, $mday, $hour, $min, $sec) = (localtime)[5,4,3,2,1,0];
    return sprintf("otter_sub_backup_%04d-%02d-%02d_%02d-%02d-%02d",
        $year + 1900, $mon + 1, $mday, $hour, $min, $sec);
}

sub cleanup_old_otter_dbs {
    my( $dir ) = @_;
    
    opendir BACKUPS, $dir or die "Can't opendir '$dir' : $!";
    my @all = sort grep { /^otter_sub_backup_/ } readdir BACKUPS;
    closedir BACKUPS;
    
    # Keep the most recent 12
    foreach my $i (1..12) {
        my $backup = pop(@all);
        warn sprintf "Keeping %02d: %s\n", $i, $backup;
        last unless @all;
    }
    
    my( %seen_month );
    # Keep one from every month
    foreach my $backup (@all) {
        my ($year_month) = $backup =~ /(\d{4}-\d\d)/;
        if ($seen_month{$year_month}) {
            print STDERR "Deleteting old backup: $backup\n";
            rmtree("$dir/$backup");
        } else {
            print STDERR "Keeping: $backup\n";
            $seen_month{$year_month} = $backup;
        }
    }

    return;
}

__END__

=head1 NAME - backup_otter_submissions_db

=head1 AUTHOR

Ana Code B<email> anacode@sanger.ac.uk

