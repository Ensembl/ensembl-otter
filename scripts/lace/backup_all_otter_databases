#!/usr/local/bin/perl -w

### backup_all_otter_databases

use strict;
use DBI;
use File::Path 'rmtree';

{
    umask(002);
    {
        my $path = $ENV{'PATH'};
        $ENV{'PATH'} = "/usr/local/bin:/usr/apps/bin:$path";
    }

    my $user = 'root';
    my $host = 'humsrv1';
    # Can add password with netrc if needed
    my $socket = '/tmp/mysql4.sock';
    my $archive = '/nfs/disk100/humpub/data/mysql_backup/otter';
    my $root_dir = '/humsql/otter';
    my $date_dir = date_dir();
    my $working_dir = "$root_dir/$date_dir";
    mkdir($working_dir) or die "Can't create '$working_dir' directory : $!";
    chdir($working_dir) or die "Can't chdir to '$working_dir' : $!";
    
    my $dbh = DBI->connect(
        "DBI:mysql:mysql_socket=$socket",
        $user, undef, {RaiseError => 1});
    my $dbs = list_otter_databases($dbh);
    
    my $mysql_dump = "mysqldump --no-defaults --opt --flush-logs --socket=$socket --user=$user";
    
    my $err_count = 0;
    my $backup_start = time;
    foreach my $db_name (@$dbs) {
        eval {
            print STDERR "Backing up '$db_name'\n";
            $dbh->do("use $db_name");
            check_database_not_too_big($dbh);
            backup_otter_database($dbh, $mysql_dump, $db_name);
        };
        if ($@) {
            $err_count++;
            print STDERR $@;
        }
    }
    my $backup_end = time;
    printf STDERR "otter backup took %d minutes\n", ($backup_end - $backup_start) / 60;
    
    chdir($root_dir) or die "Can't chdir to '$root_dir' : $!";
    my $copy = "gtar cf - $date_dir | ssh cbi1 gtar xf - -C $archive";
    if (system($copy) == 0) {
        rmtree($date_dir);
    } else {
        print STDERR "ERROR: exit $? from copy command: '$copy'\n";
        $err_count++;
    }
    my $copy_end = time;
    printf STDERR "otter copy took %d minutes\n", ($copy_end - $backup_end) / 60;
    
    ### Might want to remove old backups here, which could be
    ### done via an ssh command to take NFS out of the loop.
    
    # All errors reflected in the exit status
    exit $err_count;
}

sub check_database_not_too_big {
    my( $dbh ) = @_;
    
    my $row_count = 0;
    foreach my $table (qw{
        dna_align_feature
        protein_align_feature
        simple_feature
        })
    {
        eval{
            my $sth = $dbh->prepare(qq{ SELECT count(*) FROM $table });
            $sth->execute;
            my ($rows) = $sth->fetchrow;
            $row_count += $rows;
        };
        warn $@ if $@;
    }
    my $max_rows = 100_000;
    if ($row_count > $max_rows) {
        die "ERROR: Database has $row_count rows in feature tables: limit is $max_rows\n";
    }
}

sub backup_otter_database {
    my( $dbh, $mysql_dump, $db_name ) = @_;
    
    my $sth = $dbh->prepare("show tables");
    $sth->execute;
    my( @tables_to_backup );
    while (my ($tab) = $sth->fetchrow) {
        # The DNA table has a very large amount of data
        # in it, but we don't need to back it up.
        next if $tab eq 'dna';
        push(@tables_to_backup, $tab);
    }
    
    my $sql_file = "$db_name.sql";
    my $dump_command = "$mysql_dump $db_name @tables_to_backup > $sql_file";
    system($dump_command) == 0 or die "ERROR: exit $? running '$dump_command'\n";
}

sub list_otter_databases {
    my( $dbh ) = @_;
    
    my $sth = $dbh->prepare('show databases');
    $sth->execute;
    
    my $dbs = [];
    while (my ($db_name) = $sth->fetchrow) {
        if ($db_name =~ /^otter/i) {
            push(@$dbs, $db_name);
        } else {
            print STDERR "Skipping database '$db_name'\n";
        }
    }
    return $dbs;
}

sub date_dir {
    my( $dbh ) = @_;
    
    my ($year, $mon, $mday, $hour, $min, $sec) = (localtime)[5,4,3,2,1,0];
    return sprintf("otter_backup_%04d-%02d-%02d_%02d-%02d-%02d",
        $year + 1900, $mon + 1, $mday, $hour, $min, $sec);
}

__END__

=head1 NAME - backup_all_otter_databases

=head1 AUTHOR

James Gilbert B<email> jgrg@sanger.ac.uk

